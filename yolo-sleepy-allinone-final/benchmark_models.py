#!/usr/bin/env python3
"""
Benchmark script ƒë·ªÉ so s√°nh hi·ªáu su·∫•t YOLOv5 vs v8 vs v11
"""

import time
import cv2
import numpy as np
from ultralytics import YOLO
import psutil
import os
from pathlib import Path

class YOLOBenchmark:
    def __init__(self):
        self.results = {}
        
    def create_test_images(self, num_images=10):
        """T·∫°o t·∫≠p ·∫£nh test"""
        print(f"üì∏ T·∫°o {num_images} ·∫£nh test...")
        images = []
        
        for i in range(num_images):
            # T·∫°o ·∫£nh 640x640 v·ªõi noise v√† h√¨nh ng∆∞·ªùi
            img = np.random.randint(50, 150, (640, 640, 3), dtype=np.uint8)
            
            # Th√™m h√¨nh ng∆∞·ªùi ƒë∆°n gi·∫£n ·ªü v·ªã tr√≠ ng·∫´u nhi√™n
            x_offset = np.random.randint(100, 400)
            y_offset = np.random.randint(50, 200)
            
            # ƒê·∫ßu
            cv2.circle(img, (x_offset, y_offset), 25, (255, 200, 200), -1)
            
            # Th√¢n
            cv2.rectangle(img, (x_offset-20, y_offset+25), (x_offset+20, y_offset+150), (200, 255, 200), -1)
            
            # Tay
            cv2.rectangle(img, (x_offset-40, y_offset+40), (x_offset-20, y_offset+100), (200, 200, 255), -1)
            cv2.rectangle(img, (x_offset+20, y_offset+40), (x_offset+40, y_offset+100), (200, 200, 255), -1)
            
            images.append(img)
        
        return images
    
    def benchmark_model(self, model_name, model_path, test_images):
        """Benchmark m·ªôt model c·ª• th·ªÉ"""
        print(f"\nüß™ Benchmarking {model_name}...")
        
        try:
            # Load model
            start_load = time.time()
            model = YOLO(model_path)
            load_time = time.time() - start_load
            
            print(f"‚è±Ô∏è  Load time: {load_time:.3f}s")
            
            # Warm up
            warmup_img = test_images[0]
            model(warmup_img, verbose=False)
            
            # Benchmark inference
            inference_times = []
            total_detections = 0
            
            # Memory usage tr∆∞·ªõc khi test
            process = psutil.Process(os.getpid())
            mem_before = process.memory_info().rss / 1024 / 1024  # MB
            
            for img in test_images:
                start_inf = time.time()
                results = model(img, verbose=False)
                inf_time = time.time() - start_inf
                
                inference_times.append(inf_time)
                
                # ƒê·∫øm s·ªë detections
                if results and len(results) > 0:
                    r = results[0]
                    if hasattr(r, 'boxes') and r.boxes is not None:
                        total_detections += len(r.boxes)
            
            # Memory usage sau test
            mem_after = process.memory_info().rss / 1024 / 1024  # MB
            mem_usage = mem_after - mem_before
            
            # T√≠nh to√°n metrics
            avg_inference = np.mean(inference_times)
            fps = 1.0 / avg_inference
            total_time = sum(inference_times)
            
            results = {
                'model_name': model_name,
                'load_time': load_time,
                'avg_inference_time': avg_inference,
                'fps': fps,
                'total_time': total_time,
                'total_detections': total_detections,
                'memory_usage_mb': mem_usage,
                'min_inference': min(inference_times),
                'max_inference': max(inference_times)
            }
            
            print(f"üìä Results for {model_name}:")
            print(f"   Load time: {load_time:.3f}s")
            print(f"   Avg inference: {avg_inference:.3f}s")
            print(f"   FPS: {fps:.1f}")
            print(f"   Total detections: {total_detections}")
            print(f"   Memory usage: {mem_usage:.1f} MB")
            
            return results
            
        except Exception as e:
            print(f"‚ùå L·ªói v·ªõi {model_name}: {e}")
            return None
    
    def run_benchmark(self):
        """Ch·∫°y benchmark cho t·∫•t c·∫£ models"""
        print("üöÄ YOLO Model Benchmark")
        print("=" * 50)
        
        # T·∫°o test images
        test_images = self.create_test_images(20)
        
        # ƒê·ªãnh nghƒ©a models ƒë·ªÉ test
        models_to_test = [
            ("YOLOv5n-pose", "yolov5nu.pt"),
            ("YOLOv8n-pose", "yolov8n-pose.pt"),  
            ("YOLOv11n-pose", "yolo11n-pose.pt"),
        ]
        
        all_results = []
        
        for model_name, model_path in models_to_test:
            result = self.benchmark_model(model_name, model_path, test_images)
            if result:
                all_results.append(result)
        
        # So s√°nh k·∫øt qu·∫£
        self.compare_results(all_results)
    
    def compare_results(self, results):
        """So s√°nh k·∫øt qu·∫£ c√°c models"""
        print("\nüìà COMPARISON RESULTS")
        print("=" * 60)
        
        if not results:
            print("‚ùå Kh√¥ng c√≥ k·∫øt qu·∫£ ƒë·ªÉ so s√°nh")
            return
        
        # T√¨m model t·ªët nh·∫•t cho t·ª´ng metric
        best_fps = max(results, key=lambda x: x['fps'])
        best_load = min(results, key=lambda x: x['load_time'])
        best_memory = min(results, key=lambda x: x['memory_usage_mb'])
        
        # In b·∫£ng so s√°nh
        print(f"{'Model':<15} {'FPS':<8} {'Load(s)':<8} {'Memory(MB)':<10} {'Detections':<12}")
        print("-" * 60)
        
        for result in results:
            name = result['model_name']
            fps = result['fps']
            load_time = result['load_time']
            memory = result['memory_usage_mb']
            detections = result['total_detections']
            
            print(f"{name:<15} {fps:<8.1f} {load_time:<8.3f} {memory:<10.1f} {detections:<12}")
        
        print("\nüèÜ WINNERS:")
        print(f"   Fastest FPS: {best_fps['model_name']} ({best_fps['fps']:.1f} FPS)")
        print(f"   Fastest Load: {best_load['model_name']} ({best_load['load_time']:.3f}s)")
        print(f"   Least Memory: {best_memory['model_name']} ({best_memory['memory_usage_mb']:.1f} MB)")
        
        # Recommendation
        print("\nüí° RECOMMENDATIONS:")
        if best_fps['model_name'] == best_memory['model_name']:
            print(f"   ü•á Overall Winner: {best_fps['model_name']} (Best FPS + Memory)")
        else:
            print(f"   ‚ö° For Speed: {best_fps['model_name']}")
            print(f"   üíæ For Memory: {best_memory['model_name']}")


def main():
    """Main function"""
    benchmark = YOLOBenchmark()
    benchmark.run_benchmark()
    
    print("\n‚ú® Benchmark completed!")
    print("üí° B·∫°n c√≥ th·ªÉ ch·ªçn model ph√π h·ª£p d·ª±a tr√™n k·∫øt qu·∫£ tr√™n")


if __name__ == "__main__":
    main()