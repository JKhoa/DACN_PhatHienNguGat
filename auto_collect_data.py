#!/usr/bin/env python3
"""
Script thu tháº­p tá»± Ä‘á»™ng hÃ¬nh áº£nh vÃ  video vá» há»c sinh ngá»§ gáº­t
Sá»­ dá»¥ng nhiá»u nguá»“n: Pexels, Unsplash, Pixabay
"""

import os
import re
import requests
import time
import json
from pathlib import Path
import urllib.parse

class AutoDataCollector:
    def __init__(self):
        self.base_dir = Path("data_raw")
        self.auto_collected_dir = self.base_dir / "auto_collected"
        self.auto_collected_dir.mkdir(parents=True, exist_ok=True)
        
        # Táº¡o thÆ° má»¥c phÃ¢n loáº¡i
        self.pexels_dir = self.auto_collected_dir / "pexels"
        self.unsplash_dir = self.auto_collected_dir / "unsplash"
        self.pixabay_dir = self.auto_collected_dir / "pixabay"
        
        for dir_path in [self.pexels_dir, self.unsplash_dir, self.pixabay_dir]:
            dir_path.mkdir(exist_ok=True)
    
    def get_pexels_search_results(self, query, per_page=30):
        """TÃ¬m kiáº¿m áº£nh tá»« Pexels qua scraping"""
        print(f"ğŸ” Searching Pexels for: '{query}'")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        # URL tÃ¬m kiáº¿m Pexels
        search_url = f"https://www.pexels.com/search/{urllib.parse.quote(query)}/"
        
        try:
            response = requests.get(search_url, headers=headers)
            if response.status_code == 200:
                content = response.text
                
                # TÃ¬m cÃ¡c URL áº£nh trong HTML
                # Pexels sá»­ dá»¥ng pattern nhÆ°: "srcSet":"url1 1x, url2 2x"
                pattern = r'"src":"(https://images\.pexels\.com/photos/\d+/[^"]*\.jpeg[^"]*)"'
                matches = re.findall(pattern, content)
                
                # Lá»c vÃ  lÃ m sáº¡ch URLs
                image_urls = []
                for match in matches[:per_page]:
                    # ThÃªm parameters Ä‘á»ƒ láº¥y áº£nh cháº¥t lÆ°á»£ng cao
                    if '?' not in match:
                        match += "?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=2"
                    elif 'w=' not in match:
                        match += "&w=1260&h=750&dpr=2"
                    
                    image_urls.append(match)
                
                print(f"âœ… Found {len(image_urls)} images for '{query}'")
                return list(set(image_urls))  # Remove duplicates
                
        except Exception as e:
            print(f"âŒ Error searching Pexels: {e}")
        
        return []
    
    def get_unsplash_search_results(self, query, per_page=20):
        """TÃ¬m kiáº¿m áº£nh tá»« Unsplash"""
        print(f"ğŸ” Searching Unsplash for: '{query}'")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        search_url = f"https://unsplash.com/s/photos/{urllib.parse.quote(query)}"
        
        try:
            response = requests.get(search_url, headers=headers)
            if response.status_code == 200:
                content = response.text
                
                # TÃ¬m URLs áº£nh Unsplash
                pattern = r'"regular":"(https://images\.unsplash\.com/photo-[^"]*)"'
                matches = re.findall(pattern, content)
                
                image_urls = []
                for match in matches[:per_page]:
                    # ThÃªm parameters cho áº£nh cháº¥t lÆ°á»£ng cao
                    if '?' not in match:
                        match += "?auto=format&fit=crop&w=1000&q=80"
                    image_urls.append(match)
                
                print(f"âœ… Found {len(image_urls)} images for '{query}'")
                return list(set(image_urls))
                
        except Exception as e:
            print(f"âŒ Error searching Unsplash: {e}")
        
        return []
    
    def download_image(self, url, filepath, source="unknown"):
        """Download áº£nh vá»›i retry logic"""
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Accept-Language': 'en-US,en;q=0.9',
        }
        
        for attempt in range(3):
            try:
                print(f"ğŸ“¥ Downloading from {source}: {filepath.name} (attempt {attempt + 1}/3)")
                
                response = requests.get(url, headers=headers, timeout=30, stream=True)
                
                if response.status_code == 200:
                    with open(filepath, 'wb') as f:
                        for chunk in response.iter_content(chunk_size=8192):
                            if chunk:
                                f.write(chunk)
                    
                    # Kiá»ƒm tra kÃ­ch thÆ°á»›c file
                    file_size = filepath.stat().st_size
                    if file_size > 10000:  # > 10KB
                        print(f"âœ… Downloaded: {filepath.name} ({file_size:,} bytes)")
                        return True
                    else:
                        print(f"âŒ File too small: {filepath.name}")
                        filepath.unlink()
                else:
                    print(f"âŒ HTTP {response.status_code}")
                    
            except Exception as e:
                print(f"âŒ Error: {e}")
                
            if attempt < 2:
                time.sleep(2)
        
        return False
    
    def collect_from_source(self, source, queries, max_per_query=15):
        """Thu tháº­p tá»« má»™t nguá»“n cá»¥ thá»ƒ"""
        print(f"\n{'='*50}")
        print(f"ğŸ¯ Collecting from {source.upper()}")
        print(f"{'='*50}")
        
        if source == "pexels":
            search_func = self.get_pexels_search_results
            output_dir = self.pexels_dir
        elif source == "unsplash":
            search_func = self.get_unsplash_search_results
            output_dir = self.unsplash_dir
        else:
            print(f"âŒ Unknown source: {source}")
            return 0
        
        total_downloaded = 0
        
        for query in queries:
            print(f"\nğŸ” Query: '{query}'")
            
            # TÃ¬m kiáº¿m URLs
            image_urls = search_func(query, per_page=max_per_query)
            
            if not image_urls:
                print(f"âš ï¸ No images found for '{query}'")
                continue
            
            # Download images
            query_downloaded = 0
            for i, url in enumerate(image_urls[:max_per_query]):
                filename = output_dir / f"{source}_{query.replace(' ', '_')}_{i+1:03d}.jpg"
                
                if self.download_image(url, filename, source):
                    query_downloaded += 1
                    total_downloaded += 1
                
                # Rate limiting
                time.sleep(1)
            
            print(f"ğŸ“Š Downloaded {query_downloaded} images for '{query}'")
            
            # Delay between queries
            time.sleep(2)
        
        print(f"\nâœ… Total downloaded from {source}: {total_downloaded}")
        return total_downloaded
    
    def auto_collect_comprehensive(self):
        """Thu tháº­p toÃ n diá»‡n tá»« nhiá»u nguá»“n"""
        print("ğŸš€ Starting Comprehensive Auto Collection")
        print("=" * 60)
        
        # Keywords Ä‘á»ƒ tÃ¬m kiáº¿m
        sleep_keywords = [
            "student sleeping",
            "tired student", 
            "sleepy classroom",
            "sleep desk",
            "student nap",
            "drowsy student",
            "exhausted student",
            "sleepy person",
            "sleep at work",
            "tired at desk",
            "napping office",
            "sleeping library"
        ]
        
        total_collected = 0
        
        # Thu tháº­p tá»« Pexels
        pexels_count = self.collect_from_source("pexels", sleep_keywords[:8], max_per_query=12)
        total_collected += pexels_count
        
        # Thu tháº­p tá»« Unsplash  
        unsplash_count = self.collect_from_source("unsplash", sleep_keywords[4:], max_per_query=10)
        total_collected += unsplash_count
        
        print(f"\nğŸ‰ COLLECTION COMPLETE!")
        print(f"ğŸ“Š Total images collected: {total_collected}")
        print(f"  - Pexels: {pexels_count}")
        print(f"  - Unsplash: {unsplash_count}")
        
        return total_collected
    
    def copy_to_data_raw(self):
        """Sao chÃ©p táº¥t cáº£ áº£nh Ä‘Ã£ thu tháº­p vá» data_raw"""
        print(f"\nğŸ“ Copying collected images to data_raw...")
        
        copied = 0
        
        # Copy tá»« táº¥t cáº£ thÆ° má»¥c nguá»“n
        for source_dir in [self.pexels_dir, self.unsplash_dir, self.pixabay_dir]:
            for img_file in source_dir.glob("*.jpg"):
                dest = self.base_dir / f"auto_{img_file.name}"
                
                if not dest.exists():
                    import shutil
                    shutil.copy2(img_file, dest)
                    copied += 1
        
        print(f"âœ… Copied {copied} new images to data_raw")
        return copied

def main():
    """Main function"""
    print("[AUTO] Data Collector for Sleepy Detection")
    print("Collecting images from multiple free sources...")
    print("=" * 60)
    
    collector = AutoDataCollector()
    
    # Thu tháº­p tá»± Ä‘á»™ng
    total_images = collector.auto_collect_comprehensive()
    
    if total_images > 0:
        # Sao chÃ©p vá» data_raw
        copied = collector.copy_to_data_raw()
        
        print(f"\nğŸ¯ FINAL RESULTS:")
        print(f"ğŸ“¥ Images collected: {total_images}")
        print(f"ğŸ“ Images copied to data_raw: {copied}")
        print(f"ğŸ“ Auto-collected images saved in: {collector.auto_collected_dir}")
        
        print(f"\nğŸš€ Next steps:")
        print(f"1. Run auto-labeling: python collect_data.py --auto-label")
        print(f"2. Check stats: python collect_data.py --stats")
        print(f"3. Train model: cd yolo-sleepy-allinone-final/tools && python train_pose.py")
        
    else:
        print("\nâŒ No images collected. Please check internet connection and try again.")

if __name__ == "__main__":
    main()